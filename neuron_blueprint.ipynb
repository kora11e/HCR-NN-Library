{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68b66aa2",
      "metadata": {
        "id": "68b66aa2"
      },
      "source": [
        "<h2>Implementacja modelu HCRNN do postaci Neurona</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96a026e",
      "metadata": {
        "id": "e96a026e"
      },
      "source": [
        "Artykuł naukowy:\n",
        "Instniejace, podobne rozwiązania:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62247169",
      "metadata": {
        "id": "62247169"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math, os\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "334bf9ff",
      "metadata": {
        "id": "334bf9ff"
      },
      "outputs": [],
      "source": [
        "__all__ = [\n",
        "    'HCRNN_Neuron'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d1705e5e",
      "metadata": {
        "id": "d1705e5e"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "\n",
        "'''\n",
        "Klasa rozszerzająca funkcje torch'a.\n",
        "'''\n",
        "\n",
        "class HCRNN_Neuron(nn.Module):\n",
        "\n",
        "    '''\n",
        "    definicja stałych określająca rozmiar wejścia i wyjścia dla liczb neuronów.\n",
        "    '''\n",
        "\n",
        "    __constants__ = ['in_features', 'out_features']\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "\n",
        "        #definicja urządzenia, CPU, GPU lub TPU\n",
        "        device = None,\n",
        "        dtype= None,\n",
        "\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "\n",
        "    #Tensor wchodzi i zawsze Tensor wychodzi\n",
        "    def forward(self, input:Tensor) -> Tensor:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d07ef1a",
      "metadata": {
        "id": "9d07ef1a"
      },
      "source": [
        "<h2>Normalization</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ef073368",
      "metadata": {
        "id": "ef073368"
      },
      "outputs": [],
      "source": [
        "class CDFNorm(nn.Module):\n",
        "    def __init__(self, method='gaussian', unbiased=True, eps=1e-5, affine=False, track_running_stats=True):\n",
        "        \"\"\"\n",
        "        Normalizacja CDF (dystrybuanty).\n",
        "\n",
        "        Parametry:\n",
        "            method: metoda normalizacji ('gaussian' lub 'empirical')\n",
        "            unbiased: czy użyć nieobciążonego estymatora wariancji\n",
        "            eps: mała wartość dla stabilności numerycznej\n",
        "            affine: czy zastosować transformację afiniczną\n",
        "            track_running_stats: czy śledzić statystyki podczas uczenia\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.method = method\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.ones(1))  # Parametr skalujący\n",
        "            self.bias = nn.Parameter(torch.zeros(1))    # Parametr przesunięcia\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            # Rejestracja buforów dla średniej i wariancji\n",
        "            self.register_buffer('running_mean', torch.zeros(1))\n",
        "            self.register_buffer('running_var', torch.ones(1))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def _gaussian_transform(self, x):\n",
        "        \"\"\"Transformacja Gaussa - normalizacja przy użyciu CDF rozkładu normalnego.\"\"\"\n",
        "        if self.training and self.track_running_stats:\n",
        "            # Obliczanie statystyk podczas uczenia\n",
        "            mean = x.mean()\n",
        "            var = x.var(unbiased=self.unbiased)\n",
        "            with torch.no_grad():\n",
        "                # Aktualizacja średniej kroczącej\n",
        "                self.running_mean = (1 - 0.1) * self.running_mean + 0.1 * mean\n",
        "                # Aktualizacja wariancji kroczącej\n",
        "                self.running_var = (1 - 0.1) * self.running_var + 0.1 * var\n",
        "                self.num_batches_tracked += 1\n",
        "        else:\n",
        "            # Użycie zapisanych statystyk podczas ewaluacji\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # Obliczenie CDF przy użyciu funkcji błędu\n",
        "        x_norm = 0.5 * (1 + torch.erf((x - mean) / (torch.sqrt(var + self.eps) * math.sqrt(2))))\n",
        "\n",
        "        if self.affine:\n",
        "            # Transformacja afiniczną\n",
        "            x_norm = x_norm * self.weight + self.bias\n",
        "\n",
        "        return x_norm\n",
        "\n",
        "    def _empirical_transform(self, x):\n",
        "        \"\"\"Empiryczna transformacja CDF na podstawie rang.\"\"\"\n",
        "        x_norm = torch.zeros_like(x)\n",
        "        for i in range(len(x)):\n",
        "            # Obliczenie rangi dla każdego elementu\n",
        "            x_norm[i] = (x < x[i]).float().mean()\n",
        "\n",
        "        if self.affine:\n",
        "            # Transformacja afiniczną\n",
        "            x_norm = x_norm * self.weight + self.bias\n",
        "\n",
        "        return x_norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Przebieg forward normalizacji CDF.\n",
        "\n",
        "        Parametry:\n",
        "            x: tensor wejściowy\n",
        "\n",
        "        Zwraca:\n",
        "            Znormalizowany tensor w przedziale [0,1]\n",
        "        \"\"\"\n",
        "        if self.method == 'gaussian':\n",
        "            return self._gaussian_transform(x)\n",
        "        elif self.method == 'empirical':\n",
        "            return self._empirical_transform(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Niewspierana metoda normalizacji: {self.method}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baza ortonormalnych wielomianów Legendre'a"
      ],
      "metadata": {
        "id": "zkFrD-QadCAn"
      },
      "id": "zkFrD-QadCAn"
    },
    {
      "cell_type": "code",
      "source": [
        "class OrthonormalLegendreBasis(nn.Module):\n",
        "    def __init__(self, max_degree: int):\n",
        "        \"\"\"\n",
        "        Implementacja ortonormalnych wielomianów Legendre'a na przedziale [0,1].\n",
        "\n",
        "        Parametry:\n",
        "            max_degree: maksymalny stopień wielomianu (0 do 3)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        # Współczynniki wielomianów Legendre'a przesuniętych na przedział [0,1]\n",
        "        # Każdy wiersz odpowiada kolejnemu wielomianowi (P0, P1, P2, P3)\n",
        "        self.register_buffer('legendre_coeffs', torch.tensor([\n",
        "            [1, 0, 0, 0],        # P0(x) = 1\n",
        "            [-1, 2, 0, 0],       # P1(x) = 2x - 1\n",
        "            [1, -6, 6, 0],       # P2(x) = 6x² - 6x + 1\n",
        "            [-1, 12, -30, 20]    # P3(x) = 20x³ - 30x² + 12x - 1\n",
        "        ], dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Oblicza wartości wielomianów Legendre'a dla zadanego wejścia.\n",
        "\n",
        "        Parametry:\n",
        "            x: tensor wejściowy w przedziale [0,1]\n",
        "\n",
        "        Zwraca:\n",
        "            Tensor z wartościami wielomianów znormalizowanych do normy L2\n",
        "        \"\"\"\n",
        "        # Zabezpieczenie przed wartościami spoza przedziału [0,1]\n",
        "        x = x.float().clamp(0, 1)\n",
        "\n",
        "        # Obliczenie potęg x: [x^0, x^1, x^2, x^3]\n",
        "        powers = torch.stack([x**i for i in range(4)], dim=-1)\n",
        "\n",
        "        # Obliczenie wartości wielomianów poprzez iloczyn współczynników i potęg\n",
        "        legendre = torch.einsum('...i,ji->...j', powers, self.legendre_coeffs)\n",
        "\n",
        "        # Normalizacja do normy L2 i wybór odpowiednich stopni wielomianów\n",
        "        return legendre[..., :self.max_degree+1] / math.sqrt(2.0)\n"
      ],
      "metadata": {
        "id": "isO9ucRIbMlp"
      },
      "id": "isO9ucRIbMlp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1de67962",
      "metadata": {
        "id": "1de67962"
      },
      "source": [
        "<h2>Joint Distribution</h2>\n",
        "Uogólniona funkcja dla wymiarów 2 i 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "239ccf8b",
      "metadata": {
        "id": "239ccf8b"
      },
      "outputs": [],
      "source": [
        "class JointDistribution(nn.Module):\n",
        "    def __init__(self, dim, basis_size=4):\n",
        "        \"\"\"\n",
        "        Implementacja rozkładu łącznego przy użyciu wielomianów Legendre'a.\n",
        "\n",
        "        Parametry:\n",
        "            dim: wymiarowość rozkładu (2 lub 3)\n",
        "            basis_size: rozmiar bazy funkcji ortogonalnych\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.basis_size = basis_size\n",
        "\n",
        "        # Tensor współczynników rozkładu łącznego\n",
        "        self.coeffs = nn.Parameter(torch.zeros(*(basis_size for _ in range(dim))))\n",
        "\n",
        "        # Baza wielomianów Legendre'a\n",
        "        self.basis = OrthonormalLegendreBasis(basis_size - 1)\n",
        "\n",
        "    def forward(self, *inputs: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Oblicza wartość rozkładu łącznego dla zadanych wejść.\n",
        "\n",
        "        Parametry:\n",
        "            *inputs: tensory wejściowe (2 lub 3 w zależności od dim)\n",
        "\n",
        "        Zwraca:\n",
        "            Wartość rozkładu łącznego dla zadanych punktów\n",
        "        \"\"\"\n",
        "        # Obliczenie wartości funkcji bazowych dla każdego wejścia\n",
        "        basis_values = [self.basis(x).squeeze() for x in inputs]  # Usunięcie nadmiarowych wymiarów\n",
        "\n",
        "        if self.dim == 2:\n",
        "            # Rozkład 2D: suma po i,j (coeffs[i,j] * basis_i(x) * basis_j(y))\n",
        "            return torch.einsum('i,j,ij->', *basis_values, self.coeffs).expand(inputs[0].shape[0])\n",
        "        elif self.dim == 3:\n",
        "            # Rozkład 3D: suma po i,j,k (coeffs[i,j,k] * basis_i(x) * basis_j(y) * basis_k(z))\n",
        "            return torch.einsum('i,j,k,ijk->', *basis_values, self.coeffs).expand(inputs[0].shape[0])\n",
        "        else:\n",
        "            raise ValueError(f\"Niewspierana wymiarowość: {self.dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6685b455",
      "metadata": {
        "id": "6685b455"
      },
      "source": [
        "<h2>Estymacja średnich</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "55d288cf",
      "metadata": {
        "id": "55d288cf"
      },
      "outputs": [],
      "source": [
        "class Estimation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 *,\n",
        "                 triplets,\n",
        "                 feature_fn,\n",
        "                 feature_dm\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.triplets = triplets,\n",
        "        self.feature_fn = feature_fn,\n",
        "        self.feature_dm - feature_dm\n",
        "\n",
        "    def compute_tensor_mean(self) -> Tensor:\n",
        "        \"\"\"\n",
        "        Parametry:\n",
        "            triplets: array (x, y, z)\n",
        "            feature_fn: funckaj mapująca\n",
        "            feature_dim: wymiary D\n",
        "        \"\"\"\n",
        "        a = np.zeros((self.feature_dim, self.feature_dim, self.feature_dim))\n",
        "\n",
        "        for (x, y, z) in self.triplets:\n",
        "            fx = self.feature_fn(x)\n",
        "            fy = self.feature_fn(y)\n",
        "            fz = self.feature_fn(z)\n",
        "\n",
        "            outer = np.einsum(fx, fy, fz)\n",
        "\n",
        "            a += outer\n",
        "\n",
        "        a /= len(self.triplets)  # Normalizacja na trójkach\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fa0fa5",
      "metadata": {
        "id": "73fa0fa5"
      },
      "source": [
        "<h2>Estymacja warunkowa</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e4a4856",
      "metadata": {
        "id": "1e4a4856"
      },
      "outputs": [],
      "source": [
        "class ConditionalEstimation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 *,\n",
        "                 x_candidates,\n",
        "                 y,\n",
        "                 z,\n",
        "                 a,\n",
        "                 feature_fn) -> None:\n",
        "        super().__init__()\n",
        "        self.x_candidates = x_candidates,\n",
        "        self.y = y,\n",
        "        self.z = z,\n",
        "        self.a = a,\n",
        "        self.feature_fn - feature_fn\n",
        "\n",
        "    def conditional_score(self):\n",
        "\n",
        "        D = self.a.shape[0]\n",
        "        fy = self.feature_fn(self.y)\n",
        "        fz = self.feature_fn(self.z)\n",
        "\n",
        "        denominator = 0\n",
        "        for j in range(D):\n",
        "            for k in range(D):\n",
        "                denominator += self.fa[0, j, k] * fy[j] * fz[k]\n",
        "\n",
        "        scores = []\n",
        "        for x in self.x_candidates:\n",
        "            fx = self.feature_fn(x)\n",
        "\n",
        "            score = 0\n",
        "            for i in range(D):\n",
        "                context_sum = 0\n",
        "                for j in range(D):\n",
        "                    for k in range(D):\n",
        "                        context_sum += self.a[i, j, k] * fy[j] * fz[k]\n",
        "                score += fx[i] * (context_sum / (denominator + 1e-8)) #uniknięcie dzielenia przez zero\n",
        "\n",
        "            scores.append(score)\n",
        "\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73556b28",
      "metadata": {
        "id": "73556b28"
      },
      "source": [
        "<h2>Propagacja 1</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d0552a2b",
      "metadata": {
        "id": "d0552a2b"
      },
      "outputs": [],
      "source": [
        "class PropagationEstimation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 *,\n",
        "                 y,\n",
        "                 z,\n",
        "                 a,\n",
        "                 feature_fn):\n",
        "        super().__init__()\n",
        "        self.y = y,\n",
        "        self.z = z,\n",
        "        self.a = a,\n",
        "        self.feature_fn = feature_fn\n",
        "\n",
        "    def propagate_expectation(self):\n",
        "\n",
        "        fy = self.feature_fn(self.y)\n",
        "        fz = self.feature_fn(self.z)\n",
        "        D = fy.shape[0]\n",
        "\n",
        "        numerator = 0.0\n",
        "        denominator = 0.0\n",
        "        for j in range(D):\n",
        "            for k in range(D):\n",
        "                numerator += self.a[1, j, k] * fy[j] * fz[k]\n",
        "                denominator += self.a[0, j, k] * fy[j] * fz[k]\n",
        "\n",
        "        propagated = 0.5 + (1 / (2 * np.sqrt(3))) * (numerator / (denominator + 1e-8))\n",
        "        return propagated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db695a9",
      "metadata": {
        "id": "7db695a9"
      },
      "source": [
        "<h2>Entropia</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c5228003",
      "metadata": {
        "id": "c5228003"
      },
      "outputs": [],
      "source": [
        "class EntropyAndMutualInformation(nn.Module):\n",
        "\n",
        "    def approximate_entropy(self, activations):\n",
        "\n",
        "        # Normalizacja prawdopodobieństw funkcji aktywacji\n",
        "        probs = F.softmax(activations, dim=1)\n",
        "        entropy = -torch.sum(probs ** 2, dim=1).mean()\n",
        "        return entropy\n",
        "\n",
        "    def approximate_mutual_information(self, act_X, act_Y):\n",
        "\n",
        "        # Normalizacja funkcji aktywacji\n",
        "        probs_X = F.softmax(act_X, dim=1)\n",
        "        probs_Y = F.softmax(act_Y, dim=1)\n",
        "\n",
        "        joint_probs = torch.bmm(probs_X.unsqueeze(2), probs_Y.unsqueeze(1))\n",
        "\n",
        "        mi = torch.sum(joint_probs ** 2, dim=(1,2)).mean()\n",
        "        return mi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096dff88",
      "metadata": {
        "id": "096dff88"
      },
      "source": [
        "<h2>Dynamicznie modyfikowany model za pomocą EMA</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cace35ed",
      "metadata": {
        "id": "cace35ed"
      },
      "outputs": [],
      "source": [
        "class DynamicEMA(nn.Module):\n",
        "    def __init__(self, x, y, z, ema_lambda) -> None:\n",
        "        self.x = x,\n",
        "        self.y = y,\n",
        "        self.z = z,\n",
        "        self.ema_lambda = ema_lambda\n",
        "\n",
        "    def EMAUpdateMethod(self):\n",
        "        def f_i(x): return x\n",
        "        def f_j(y): return y\n",
        "        def f_k(z): return z\n",
        "\n",
        "        update_tensor = torch.einsum('i,j,k->ijk', f_i(self.x), f_j(self.y), f_k(self.z))\n",
        "\n",
        "        # EMA updating values\n",
        "        a = (1 - self.ema_lambda) * a + self.ema_lambda * update_tensor\n",
        "\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c770ab",
      "metadata": {
        "id": "51c770ab"
      },
      "source": [
        "<h2>Optymizacja bazy</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "254d8e2a",
      "metadata": {
        "id": "254d8e2a"
      },
      "outputs": [],
      "source": [
        "class BaseOptimization(nn.Module):\n",
        "    def __init__(self,\n",
        "                 *,\n",
        "                 a, #tensor do optymalizacji\n",
        "                 ) -> None:\n",
        "        self. a = a\n",
        "\n",
        "    def optimization_early(self) -> Tensor:\n",
        "        M = self.a.reshape(len(self.a[0]), -1)\n",
        "\n",
        "        # Obliczenie SVD\n",
        "        U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n",
        "\n",
        "        # Transformacja Bazy, tu przykładowa funkcja, do wymiany\n",
        "        def f_x(x):\n",
        "            return torch.sin(x * torch.linspace(0, 1, len(self.a[2])))\n",
        "\n",
        "        # nowa baza g_i(x) = sum_j v_ij * f_j(x)\n",
        "        def g_i(x, U):\n",
        "            f = f_x(x)\n",
        "            return torch.matmul(U.T, f)\n",
        "\n",
        "        # Step 4: Transformacja Tensora\n",
        "        new_a = torch.einsum('li,ljk->ijk', U.T, self.a)\n",
        "\n",
        "        return new_a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information bottleneck"
      ],
      "metadata": {
        "id": "8j7Cu9GqbT2J"
      },
      "id": "8j7Cu9GqbT2J"
    },
    {
      "cell_type": "code",
      "source": [
        "class InformationBottleneck(nn.Module):\n",
        "    def __init__(self, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, X_features, Y_features):\n",
        "        \"\"\"Implementuje równanie (15) z artykułu\"\"\"\n",
        "        C_X = X_features @ X_features.T\n",
        "        C_Y = Y_features @ Y_features.T\n",
        "        return torch.trace(C_X @ C_Y)\n",
        "\n",
        "    def bottleneck_loss(self, X_features, T_features, Y_features):\n",
        "        \"\"\"Implementuje równanie (10) z artykułu\"\"\"\n",
        "        I_XT = self(X_features, T_features)\n",
        "        I_TY = self(T_features, Y_features)\n",
        "        return I_XT - self.beta * I_TY"
      ],
      "metadata": {
        "id": "LdpFVrT4bWLD"
      },
      "id": "LdpFVrT4bWLD",
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}